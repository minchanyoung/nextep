@startuml NEXTEP_AI_RAG_System_Sequence

title NEXTEP.AI RAG (Retrieval-Augmented Generation) 시스템 - Sequence Diagram

actor "시스템 관리자" as Admin
participant "RAG Manager" as RAGManager
participant "Document Processor" as DocProcessor
participant "LangChain Components" as LangChain
participant "ChromaDB" as VectorDB
participant "Ollama Embedding" as EmbeddingModel
participant "PDF Documents" as PDFs
participant "Ollama LLM" as LLM
participant "Cache Manager" as Cache

== RAG 시스템 초기화 ==
Admin -> RAGManager: init_app(app, llm_service)
RAGManager -> RAGManager: _initialize_vector_store()

RAGManager -> VectorDB: create Chroma collection
VectorDB --> RAGManager: collection 생성됨

RAGManager -> EmbeddingModel: OllamaEmbeddings(model="llama3")
EmbeddingModel --> RAGManager: 임베딩 모델 로드

RAGManager -> RAGManager: _initialize_rag_chain()
RAGManager -> LangChain: create ChatPromptTemplate
LangChain --> RAGManager: RAG 프롬프트 템플릿

RAGManager -> RAGManager: create retriever
RAGManager --> Admin: RAG 시스템 초기화 완료

== PDF 문서 수집 및 처리 ==
Admin -> RAGManager: ingest_pdf_document(pdf_path)
RAGManager -> DocProcessor: process_pdf(pdf_path)

DocProcessor -> PDFs: load PDF file
PDFs --> DocProcessor: PDF 내용

DocProcessor -> LangChain: RecursiveCharacterTextSplitter
LangChain -> LangChain: chunk_size=1000, overlap=200
LangChain --> DocProcessor: text chunks

DocProcessor -> DocProcessor: extract metadata (source, page, keywords)
DocProcessor --> RAGManager: processed_documents

RAGManager -> EmbeddingModel: generate embeddings for chunks
EmbeddingModel --> RAGManager: vector embeddings

RAGManager -> VectorDB: store documents + embeddings
VectorDB --> RAGManager: 저장 완료

RAGManager --> Admin: PDF 처리 완료

== 기존 데이터 마이그레이션 ==
Admin -> RAGManager: migrate_legacy_data()
RAGManager -> RAGManager: LABOR_MARKET_TRENDS 처리

loop 각 트렌드 데이터
    RAGManager -> DocProcessor: create_document(content, metadata)
    DocProcessor --> RAGManager: Document 객체
    RAGManager -> EmbeddingModel: 텍스트 임베딩 생성
    EmbeddingModel --> RAGManager: vector
    RAGManager -> VectorDB: 벡터 저장
end

RAGManager -> RAGManager: LEARNING_RECOMMENDATIONS 처리
loop 각 학습 추천 데이터
    RAGManager -> DocProcessor: create_document(content, metadata)
    DocProcessor --> RAGManager: Document 객체
    RAGManager -> EmbeddingModel: 텍스트 임베딩 생성
    EmbeddingModel --> RAGManager: vector
    RAGManager -> VectorDB: 벡터 저장
end

RAGManager --> Admin: 기존 데이터 마이그레이션 완료

== 사용자 질의 처리 ==
participant "사용자" as User
User -> RAGManager: get_career_advice(query_text)

RAGManager -> Cache: check cached result
Cache --> RAGManager: cache miss

RAGManager -> RAGManager: preprocess_query(query_text)
RAGManager -> VectorDB: similarity_search(query, k=5, score_threshold=0.3)

VectorDB -> VectorDB: vector similarity calculation
VectorDB --> RAGManager: top_k relevant documents

RAGManager -> RAGManager: _format_docs(documents)
RAGManager -> LangChain: RAG chain invoke
LangChain -> LLM: context + query 전송
LLM -> LLM: generate contextual response
LLM --> LangChain: AI 답변
LangChain --> RAGManager: formatted response

RAGManager -> Cache: store result
RAGManager --> User: career advice

== 노동시장 동향 특화 검색 ==
User -> RAGManager: get_labor_market_info(query)
RAGManager -> Cache: check cache
Cache --> RAGManager: cache miss

RAGManager -> RAGManager: enhance query for labor market
RAGManager -> VectorDB: search with labor market filter
note right: metadata.get('source').contains('labor_market')

VectorDB --> RAGManager: labor market documents
RAGManager -> LangChain: specialized labor market prompt
LangChain -> LLM: generate labor market analysis
LLM --> LangChain: market analysis
LangChain --> RAGManager: formatted analysis

RAGManager -> Cache: store with TTL
RAGManager --> User: labor market insights

== 학습 추천 특화 검색 ==
User -> RAGManager: get_learning_recommendations(query)
RAGManager -> Cache: check cache
Cache --> RAGManager: cache miss

RAGManager -> RAGManager: enhance query for learning
RAGManager -> VectorDB: search with learning filter
note right: metadata.get('chunk_type') == 'learning'

VectorDB --> RAGManager: learning recommendation documents
RAGManager -> LangChain: specialized learning prompt
LangChain -> LLM: generate learning recommendations
LLM --> LangChain: personalized recommendations
LangChain --> RAGManager: formatted recommendations

RAGManager -> Cache: store result
RAGManager --> User: learning recommendations

== 시스템 모니터링 ==
Admin -> RAGManager: get_collection_stats()
RAGManager -> VectorDB: collection.count()
VectorDB --> RAGManager: document count
RAGManager -> Cache: get cache stats
Cache --> RAGManager: cache hit rate
RAGManager --> Admin: {count: N, cache_hit_rate: X%}

note over Admin, Cache: RAG 시스템은 3개 PDF 문서 + 기존 데이터로\n노동시장 전문 지식 베이스 구축\n임계값 0.3으로 관련성 높은 정보만 활용

@enduml