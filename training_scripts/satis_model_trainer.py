import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("data/klips_data_23.csv")

# 2. Feature / Target ì„¤ì •
features = [
    "age", "gender", "education", "monthly_income", "job_category",
    "satis_wage", "satis_stability", "satis_task_content", "satis_work_env",
    "satis_work_time", "satis_growth", "satis_communication",
    "satis_fair_eval", "satis_welfare", "prev_job_satisfaction",
    "prev_monthly_income", "job_category_income_avg",
    "income_relative_to_job", "job_category_education_avg", "education_relative_to_job",
    "job_category_satisfaction_avg", "age_x_job_category", "monthly_income_x_job_category",
    "education_x_job_category", "income_relative_to_job_x_job_category"
]

X = df[features]
y = df["satisfaction_change_score"]

# ì‚¬ìš©ìê»˜ì„œ í™•ì¸í•´ì£¼ì‹  ì‹¤ì œ yê°’ì˜ ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ ì‚¬ìš©
MIN_SATISFACTION_CHANGE = -4
MAX_SATISFACTION_CHANGE = 3
print(f"Satisfaction Change Scoreì˜ ì‹¤ì œ ìµœì†Œê°’: {MIN_SATISFACTION_CHANGE}")
print(f"Satisfaction Change Scoreì˜ ì‹¤ì œ ìµœëŒ€ê°’: {MAX_SATISFACTION_CHANGE}")


# 3. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
latest_year = df["year"].max()
X_train = X[df["year"] < latest_year]
y_train = y[df["year"] < latest_year]
X_test = X[df["year"] == latest_year]
y_test = y[df["year"] == latest_year]

# 4. XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
tscv = TimeSeriesSplit(n_splits=5)
xgb_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.05, 0.1, 0.2],
    'subsample': [0.8, 1.0]
}
xgb_search = GridSearchCV(
    estimator=XGBRegressor(random_state=42),
    param_grid=xgb_grid,
    cv=tscv,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)
xgb_search.fit(X_train, y_train)
xgb_best = xgb_search.best_estimator_
y_pred_xgb = xgb_best.predict(X_test)

# 5. CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
cat_grid = {
    'iterations': [50, 100, 150],
    'depth': [3, 4, 5],
    'learning_rate': [0.05, 0.1, 0.2]
}
cat_search = GridSearchCV(
    estimator=CatBoostRegressor(verbose=0, random_state=42),
    param_grid=cat_grid,
    cv=tscv,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)
cat_search.fit(X_train, y_train, cat_features=["gender", "education", "job_category"])
cat_best = cat_search.best_estimator_
y_pred_cat = cat_best.predict(X_test)

# 6. Soft-Blending
alpha = 0.3
y_pred_blend = alpha * y_pred_xgb + (1 - alpha) * y_pred_cat

# --- ì˜ˆì¸¡ê°’ í›„ì²˜ë¦¬: ì†Œìˆ˜ì ì„ ì •ìˆ˜ë¡œ ë³€í™˜ ë° ì‹¤ì œ ë°ì´í„° ë²”ìœ„ë¡œ í´ë¦¬í•‘ ---
# 1. ë°˜ì˜¬ë¦¼ (ê°€ì¥ ê°€ê¹Œìš´ ì •ìˆ˜)
y_pred_xgb_int = np.round(y_pred_xgb)
y_pred_cat_int = np.round(y_pred_cat)
y_pred_blend_int = np.round(y_pred_blend)

# 2. ê°’ ë²”ìœ„ ì œí•œ (í´ë¦¬í•‘): ì‚¬ìš©ìê»˜ì„œ í™•ì¸í•´ì£¼ì‹  ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ìœ¼ë¡œ ì¡°ì •
y_pred_xgb_int = np.clip(y_pred_xgb_int, MIN_SATISFACTION_CHANGE, MAX_SATISFACTION_CHANGE)
y_pred_cat_int = np.clip(y_pred_cat_int, MIN_SATISFACTION_CHANGE, MAX_SATISFACTION_CHANGE)
y_pred_blend_int = np.clip(y_pred_blend_int, MIN_SATISFACTION_CHANGE, MAX_SATISFACTION_CHANGE)
# ----------------------------------------------------

# 7. í‰ê°€ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
def evaluate(name, y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"\n-------- {name} --------")
    print("ğŸ“‰ RMSE:", round(rmse, 4))
    print("ğŸ“‰ MAE :", round(mae, 4))
    print("ğŸ“ˆ RÂ²  :", round(r2, 4))

# 8. ê²°ê³¼ ì¶œë ¥ (í›„ì²˜ë¦¬ëœ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í‰ê°€)
evaluate("XGBoost (Tuned, Integer)", y_test, y_pred_xgb_int)
evaluate("CatBoost (Tuned, Integer)", y_test, y_pred_cat_int)
evaluate("Soft-Blended Ensemble (Integer)", y_test, y_pred_blend_int)

import matplotlib.pyplot as plt
import seaborn as sns

# XGBoost Feature Importance
def plot_xgb_importance(model, feature_names, top_n=15):
    importances = model.feature_importances_
    fi = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False).head(top_n)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=fi, palette='Greens_d')
    plt.title('XGBoost Feature Importance (Top {})'.format(top_n))
    plt.tight_layout()
    plt.show()

# CatBoost Feature Importance
def plot_cat_importance(model, feature_names, top_n=15):
    importances = model.get_feature_importance()
    fi = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False).head(top_n)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=fi, palette='Blues_d')
    plt.title('CatBoost Feature Importance (Top {})'.format(top_n))
    plt.tight_layout()
    plt.show()

# í˜¸ì¶œ ì˜ˆì‹œ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ê³¼ feature list ì‚¬ìš©)
plot_xgb_importance(xgb_best, X.columns)
plot_cat_importance(cat_best, X.columns)

import joblib
import os

# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (í•„ìš” ì‹œ)
os.makedirs("app/ml/saved_models", exist_ok=True)

# 1. XGBoost ëª¨ë¸ ì €ì¥ (.pkl)
xgb_path = "app/ml/saved_models/xgb_satisfaction_change_model.pkl"
joblib.dump(xgb_best, xgb_path)
print(f"[ì €ì¥ ì™„ë£Œ] XGBoost ëª¨ë¸ â†’ {xgb_path}")

# 2. CatBoost ëª¨ë¸ ì €ì¥ (.cbm)
cat_path = "app/ml/saved_models/cat_satisfaction_change_model.cbm"
cat_best.save_model(cat_path)
print(f"[ì €ì¥ ì™„ë£Œ] CatBoost ëª¨ë¸ â†’ {cat_path}")

# 3. Soft-Blending ëª¨ë¸ì€ ë”°ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
# -> ìë°”ì—ì„œ XGBoost/CatBoost ë‘ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì˜ˆì¸¡ê°’ blending ì²˜ë¦¬